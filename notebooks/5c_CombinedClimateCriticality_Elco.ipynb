{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9072144-85f8-42b3-adb9-38249547d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party - Data and scientific computing\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyproj import Geod\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Shapely-specific imports for spatial analysis\n",
    "import shapely\n",
    "from shapely import STRtree\n",
    "from shapely.geometry import LineString, MultiLineString, Point\n",
    "from shapely.ops import nearest_points, snap\n",
    "\n",
    "# Matplotlib-specific imports for figures\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap, Normalize\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "from matplotlib.ticker import FuncFormatter, MultipleLocator\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f5d7578-9d46-4684-bcee-d04d23eb5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent\n",
    "data_path = BASE_DIR / \"input_files\"\n",
    "figure_path = BASE_DIR / \"figures\"\n",
    "intermediate_results_path = BASE_DIR / 'intermediate_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37c9c6c-a54d-4de0-a956-711fd8c7928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hazards = gpd.read_parquet(intermediate_results_path / \"main_network_hazard_exposure.parquet\") \n",
    "base_network = gpd.read_parquet(intermediate_results_path / 'PERS_directed_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0a6c4b-d2ca-4db7-8334-e8a9aa459b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Failed to open local file 'c:/Users/yma794/Documents/Serbia/Analysis - Copy2/intermediate_results/hospital_impacts.parquet'. Detail: [Windows error 2] The system cannot find the file specified.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\geopandas\\io\\arrow.py:654\u001b[39m, in \u001b[36m_read_parquet_schema_and_metadata\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     schema = \u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.schema\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1424\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1421\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1422\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1424\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1426\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1427\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\pyarrow\\dataset.py:790\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\pyarrow\\dataset.py:472\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     fs, paths_or_selector = \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    475\u001b[39m     partitioning=partitioning,\n\u001b[32m    476\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    477\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    478\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    479\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\pyarrow\\dataset.py:437\u001b[39m, in \u001b[36m_ensure_single_source\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[31mFileNotFoundError\u001b[39m: c:/Users/yma794/Documents/Serbia/Analysis - Copy2/intermediate_results/hospital_impacts.parquet",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hospital_exposed_edges = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_results_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhospital_impacts.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.to_crs(gdf_hazards.crs)\n\u001b[32m      2\u001b[39m factory_exposed_edges = gpd.read_parquet(intermediate_results_path / \u001b[33m'\u001b[39m\u001b[33mfactory_impacts.parquet\u001b[39m\u001b[33m'\u001b[39m).to_crs(gdf_hazards.crs)\n\u001b[32m      3\u001b[39m police_exposed_edges = gpd.read_parquet(intermediate_results_path / \u001b[33m'\u001b[39m\u001b[33mpolice_impacts.parquet\u001b[39m\u001b[33m'\u001b[39m).to_crs(gdf_hazards.crs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\geopandas\\io\\arrow.py:772\u001b[39m, in \u001b[36m_read_parquet\u001b[39m\u001b[34m(path, columns, storage_options, bbox, to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    768\u001b[39m filesystem, path = _get_filesystem_path(\n\u001b[32m    769\u001b[39m     path, filesystem=filesystem, storage_options=storage_options\n\u001b[32m    770\u001b[39m )\n\u001b[32m    771\u001b[39m path = _expand_user(path)\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m schema, metadata = \u001b[43m_read_parquet_schema_and_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m geo_metadata = _validate_and_decode_metadata(metadata)\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(geo_metadata[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m]) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\geopandas\\io\\arrow.py:656\u001b[39m, in \u001b[36m_read_parquet_schema_and_metadata\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    654\u001b[39m     schema = parquet.ParquetDataset(path, filesystem=filesystem, **kwargs).schema\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     schema = \u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m metadata = schema.metadata\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# read metadata separately to get the raw Parquet FileMetaData metadata\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# (pyarrow doesn't properly exposes those in schema.metadata for files\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;66;03m# created by GDAL - https://issues.apache.org/jira/browse/ARROW-16688)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2390\u001b[39m, in \u001b[36mread_schema\u001b[39m\u001b[34m(where, memory_map, decryption_properties, filesystem)\u001b[39m\n\u001b[32m   2388\u001b[39m file_ctx = nullcontext()\n\u001b[32m   2389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2390\u001b[39m     file_ctx = where = \u001b[43mfilesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_input_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2392\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx:\n\u001b[32m   2393\u001b[39m     file = ParquetFile(\n\u001b[32m   2394\u001b[39m         where, memory_map=memory_map,\n\u001b[32m   2395\u001b[39m         decryption_properties=decryption_properties)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\pyarrow\\_fs.pyx:815\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.open_input_file\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yma794\\AppData\\Local\\miniforge3\\envs\\serbia\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] Failed to open local file 'c:/Users/yma794/Documents/Serbia/Analysis - Copy2/intermediate_results/hospital_impacts.parquet'. Detail: [Windows error 2] The system cannot find the file specified.\r\n"
     ]
    }
   ],
   "source": [
    "hospital_exposed_edges = gpd.read_parquet(intermediate_results_path / 'hospital_impacts.parquet').to_crs(gdf_hazards.crs)\n",
    "factory_exposed_edges = gpd.read_parquet(intermediate_results_path / 'factory_impacts.parquet').to_crs(gdf_hazards.crs)\n",
    "police_exposed_edges = gpd.read_parquet(intermediate_results_path / 'police_impacts.parquet').to_crs(gdf_hazards.crs)\n",
    "fire_exposed_edges = gpd.read_parquet(intermediate_results_path / 'fire_impacts.parquet').to_crs(gdf_hazards.crs)\n",
    "border_exposed_edges = gpd.read_parquet(intermediate_results_path / 'road_impacts.parquet').to_crs(gdf_hazards.crs)\n",
    "port_exposed_edges = gpd.read_parquet(intermediate_results_path / 'port_impacts.parquet').to_crs(gdf_hazards.crs)\n",
    "railway_exposed_edges = gpd.read_parquet(intermediate_results_path / 'rail_impacts.parquet').to_crs(gdf_hazards.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb295a-c4f0-4513-b131-defbd86de4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_flood_change_rp =  gpd.read_parquet(intermediate_results_path / 'Future Floods change in RP.parquet').to_crs(gdf_hazards.crs)\n",
    "future_rainfall_change =  gpd.read_parquet(intermediate_results_path / 'change in maximum daily precipitation rcp 85 period 2.paquet').to_crs(gdf_hazards.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39304ec0-1e58-4f9f-a445-27dbd76a558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_impact_column(base_gdf, edges_gdf, impact_col_name, predicate=\"intersects\", agg=\"mean\", col_to_focus = 'travel_time_impact'):\n",
    "    \"\"\"\n",
    "    Spatially join edges to base_gdf, aggregate travel_time_impact per base index, and add as a new column.\n",
    "    - predicate: 'intersects', 'within', 'contains', 'touches' (choose based on geometry semantics)\n",
    "    - agg: 'mean', 'max', 'min', 'median' etc.\n",
    "    \"\"\"\n",
    "    # Keep only needed columns to avoid bloat\n",
    "    edges = edges_gdf[[col_to_focus, 'geometry']].copy()\n",
    "\n",
    "    # Spatial join (left frame is base), this creates potential many-to-one matches\n",
    "    joined = base_gdf.sjoin(edges, how='left', predicate=predicate)\n",
    "\n",
    "    # Aggregate per left index (hazard_id)\n",
    "    # Note: joined.index is the left index; the sjoin adds right index as 'index_right'\n",
    "    agg_series = joined.groupby(joined.index)[col_to_focus].agg(agg)\n",
    "\n",
    "    # Attach to base_gdf with a clear name\n",
    "    base_gdf[impact_col_name] = agg_series.reindex(base_gdf.index)\n",
    "\n",
    "    return base_gdf\n",
    "\n",
    "# Add each impact column (choose your aggregator: 'mean' or 'max')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, hospital_exposed_edges, 'hospital_delay', predicate='intersects', agg='mean')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, factory_exposed_edges,  'factory_delay',  predicate='intersects', agg='mean')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, police_exposed_edges,   'police_delay',   predicate='intersects', agg='mean')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, fire_exposed_edges,     'fire_delay',     predicate='intersects', agg='mean')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, port_exposed_edges,  'port_delay',  predicate='intersects', agg='mean')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, border_exposed_edges,   'border_delay',   predicate='intersects', agg='mean')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, railway_exposed_edges,     'railway_delay',     predicate='intersects', agg='mean')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, future_flood_change_rp,   'future_flood_change',   predicate='intersects', agg='mean',col_to_focus='rp30_mean')\n",
    "gdf_hazards = add_impact_column(gdf_hazards, future_rainfall_change,     'future_rainfall_change',     predicate='intersects', agg='mean',col_to_focus='max_rx1day_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27ea5c-b360-4549-9056-b94427c98eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLIMATE CRITICALITY METRIC - UPDATED METHODOLOGY\n",
    "# =============================================================================\n",
    "\n",
    "# Ensure gdf_hazards has a stable index\n",
    "gdf_hazards = gdf_hazards.copy()\n",
    "gdf_hazards.index.name = 'section_id'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. PREPARE DATA - Rename columns to standard names\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "gdf_hazards = gdf_hazards.rename(columns={\n",
    "    'max_depth': 'flood_depth',\n",
    "    'dužina_sn': 'snow_drift',\n",
    "    'datum_evid': 'landslide_date',\n",
    "    # Add any other renames as needed\n",
    "})\n",
    "\n",
    "# Convert landslide_date to binary presence indicator\n",
    "gdf_hazards['landslide_exposure'] = np.where(\n",
    "    gdf_hazards['landslide_date'].astype(str).str.len() > 0, 1.0, 0.0\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. DEFINE METRIC GROUPS FOR EACH SUB-INDEX\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Hazard Exposure Sub-Index (H) - 5 metrics\n",
    "hazard_metrics = [\n",
    "    'flood_depth',              # F - Maximum inundation depth (cm)\n",
    "    'future_rainfall_change',   # R - Projected % change in extreme rainfall\n",
    "    'future_flood_change',      # C - Projected % change in river flood magnitude\n",
    "    'landslide_exposure',       # L - Binary landslide exposure\n",
    "    'snow_drift'                # S - Length affected by snow drift (km)\n",
    "]\n",
    "\n",
    "# National-Scale Travel Disruption Sub-Index (T) - 4 metrics\n",
    "travel_metrics = [\n",
    "    'phl',  # Passenger hours lost\n",
    "    'thl',  # Tonnage hours lost\n",
    "    'pkl',  # Passenger kilometers lost\n",
    "    'tkl'   # Tonnage kilometers lost\n",
    "]\n",
    "\n",
    "# Local Accessibility Sub-Index (A) - 6 metrics\n",
    "accessibility_metrics = [\n",
    "    'hospital_delay',      # HOS - Hospital access delay\n",
    "    'fire_delay',          # FIR - Fire station access delay\n",
    "    'police_delay',        # POL - Police station access delay\n",
    "    'port_delay',          # PRT - Port access delay (agriculture)\n",
    "    'border_delay',        # BRD - Border crossing delay (agriculture/industry)\n",
    "    'railway_delay'        # RWY - Railway station access delay (agriculture)\n",
    "]\n",
    "\n",
    "# All metrics combined\n",
    "all_metrics = hazard_metrics + travel_metrics + accessibility_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750210e-8a90-4793-8714-b5ffa0415407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. ENSURE ALL COLUMNS EXIST AND HANDLE MISSING VALUES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "for col in all_metrics:\n",
    "    if col not in gdf_hazards.columns:\n",
    "        print(f\"Warning: Column '{col}' not found. Creating with zeros.\")\n",
    "        gdf_hazards[col] = 0.0\n",
    "    # Convert to float and fill NaN with 0\n",
    "    gdf_hazards[col] = gdf_hazards[col].astype(float).fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef761506-6211-4631-9b24-368120d605d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cd96e-e9d1-4c9f-a2a8-b7aeb7b74dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. MIN-MAX NORMALIZATION FUNCTION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def safe_minmax_normalize(series):\n",
    "    \"\"\"\n",
    "    Normalize a series to [0, 1] using min-max normalization.\n",
    "    Returns zeros if constant or all missing.\n",
    "    \"\"\"\n",
    "    s = series.astype(float).fillna(0.0)\n",
    "    min_val = s.min()\n",
    "    max_val = s.max()\n",
    "    \n",
    "    if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:\n",
    "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "    \n",
    "    return (s - min_val) / (max_val - min_val)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. NORMALIZE ALL METRICS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Normalizing metrics...\")\n",
    "\n",
    "for col in all_metrics:\n",
    "    norm_col = f'{col}_norm'\n",
    "    gdf_hazards[norm_col] = safe_minmax_normalize(gdf_hazards[col])\n",
    "    print(f\"  {col}: min={gdf_hazards[col].min():.2f}, max={gdf_hazards[col].max():.2f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. COMPUTE SUB-INDICES (Equal weights within each sub-index)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Normalized column names\n",
    "hazard_norm = [f'{col}_norm' for col in hazard_metrics]\n",
    "travel_norm = [f'{col}_norm' for col in travel_metrics]\n",
    "accessibility_norm = [f'{col}_norm' for col in accessibility_metrics]\n",
    "\n",
    "# Hazard Exposure Sub-Index: H = (1/5) × (F + R + C + L + S)\n",
    "gdf_hazards['H_hazard_exposure'] = gdf_hazards[hazard_norm].mean(axis=1)\n",
    "\n",
    "# National-Scale Travel Disruption Sub-Index: T = (1/4) × (PHL + THL + PKL + TKL)\n",
    "gdf_hazards['T_travel_disruption'] = gdf_hazards[travel_norm].mean(axis=1)\n",
    "\n",
    "# Local Accessibility Sub-Index: A = (1/6) × (HOS + FIR + POL + PRT + BRD + RWY)\n",
    "gdf_hazards['A_local_accessibility'] = gdf_hazards[accessibility_norm].mean(axis=1)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. COMPUTE COMBINED CLIMATE CRITICALITY (Equal weights across sub-indices)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# CC = (1/3) × (H + T + A)\n",
    "sub_indices = ['H_hazard_exposure', 'T_travel_disruption', 'A_local_accessibility']\n",
    "gdf_hazards['CC_climate_criticality'] = gdf_hazards[sub_indices].sum(axis=1)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. CLASSIFY INTO QUINTILES (5 categories based on 20% quantiles)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def classify_quintiles(series, labels=None):\n",
    "    if labels is None:\n",
    "        labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "    \n",
    "    # Create an empty series to store results\n",
    "    result = pd.Series(index=series.index, dtype='object')\n",
    "    \n",
    "    # 1. Identify zeros and assign \"No criticality\"\n",
    "    is_zero = (series == 0)\n",
    "    result[is_zero] = 'No criticality'\n",
    "    \n",
    "    # 2. Isolate non-zero values for quintile calculation\n",
    "    non_zeros = series[~is_zero]\n",
    "    \n",
    "    if not non_zeros.empty:\n",
    "        # Use qcut directly on non-zero values to ensure equal-sized bins\n",
    "        # duplicates='drop' handles cases with many identical non-zero values\n",
    "        quintiles = pd.qcut(non_zeros, 5, labels=labels, duplicates='drop')\n",
    "        result[~is_zero] = quintiles\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Classify each sub-index\n",
    "gdf_hazards['H_class'] = classify_quintiles(gdf_hazards['H_hazard_exposure'])\n",
    "gdf_hazards['T_class'] = classify_quintiles(gdf_hazards['T_travel_disruption'])\n",
    "gdf_hazards['A_class'] = classify_quintiles(gdf_hazards['A_local_accessibility'])\n",
    "\n",
    "# Classify combined score\n",
    "gdf_hazards['CC_class'] = classify_quintiles(gdf_hazards['CC_climate_criticality'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aade63b-4d34-43f7-aa08-d83474ea43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hazards.boxplot(['H_hazard_exposure','T_travel_disruption','A_local_accessibility','CC_climate_criticality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5186cb-45d3-4c26-b4c2-357300517fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hazards[['H_class','T_class','A_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576afb32-d086-48ec-ada5-62616bfe3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. Configuration: Colors, Labels, and Widths\n",
    "# =============================================================================\n",
    "\n",
    "labels = ['No criticality', 'Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "colors = ['#e0e0e0', '#edf8fb','#b3cde3','#8c96c6','#8856a7','#810f7c']\n",
    "color_map = dict(zip(labels, colors))\n",
    "\n",
    "width_mapping = {\n",
    "    'No criticality': 0.2,\n",
    "    'Very Low': 0.6,\n",
    "    'Low': 0.9,\n",
    "    'Medium': 1.3,\n",
    "    'High': 2.0,\n",
    "    'Very High': 3.0\n",
    "}\n",
    "\n",
    "\n",
    "# Ensure CRS is Web Mercator for contextily\n",
    "gdf_hazards = gdf_hazards.to_crs(epsg=3857)\n",
    "# If you have a separate base network for context:\n",
    "# base_network = base_network.to_crs(epsg=3857)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Plotting Function and Figure Generation\n",
    "# =============================================================================\n",
    "\n",
    "def plot_panel(ax, gdf, class_col, letter, title):\n",
    "    # Optional: Plot background network if you have one\n",
    "    # base_network.plot(ax=ax, linewidth=0.1, color='lightgrey', alpha=0.5)\n",
    "    \n",
    "    # Plot each category in specific order (lowest to highest) \n",
    "    # so high criticality is on top\n",
    "    for cat in labels:\n",
    "        subset = gdf[gdf[class_col] == cat]\n",
    "        if not subset.empty:\n",
    "            subset.plot(\n",
    "                ax=ax, \n",
    "                color=color_map[cat], \n",
    "                linewidth=width_mapping[cat],\n",
    "                alpha=0.8 if cat != 'No criticality' else 0.4,\n",
    "                zorder=labels.index(cat) # Higher criticality on top\n",
    "            )\n",
    "    \n",
    "    # Add Basemap\n",
    "    cx.add_basemap(ax, source=cx.providers.CartoDB.Positron, attribution=False)\n",
    "    \n",
    "    # UI Elements\n",
    "    ax.axis('off')\n",
    "    ax.text(0.05, 0.95, letter, transform=ax.transAxes, fontsize=22, \n",
    "            fontweight='bold', verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.9))\n",
    "    ax.set_title(title, fontsize=14,fontweight='bold')\n",
    "\n",
    "# Create the figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 7))\n",
    "\n",
    "titles = ['Hazard-Exposure', 'National-Scale Travel Disruption', 'Local Accessibility']\n",
    "\n",
    "# Loop through the axes and classification columns\n",
    "for i, ax in enumerate(axes):\n",
    "    plot_panel(axes[i], gdf_hazards, class_cols[i], chr(65+i), titles[i])\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Shared Legend\n",
    "# =============================================================================\n",
    "\n",
    "legend_handles = [\n",
    "    Patch(facecolor=color_map[lbl], label=lbl, edgecolor='grey', linewidth=0.5)\n",
    "    for lbl in labels\n",
    "]\n",
    "\n",
    "fig.legend(\n",
    "    handles=legend_handles,\n",
    "    title='Criticality Level',\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, 0.02),\n",
    "    ncol=6,\n",
    "    fontsize=13,\n",
    "    title_fontsize=15,\n",
    "    frameon=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.18)\n",
    "\n",
    "# Save or show\n",
    "plt.savefig(figure_path / 'criticality_analysis_3panel.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ed54b-7827-49ad-b0d8-c80c077721a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLIMATE CRITICALITY - SINGLE FIGURE (Mean)\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate mean criticality\n",
    "norm_cols = ['H_hazard_exposure', 'T_travel_disruption', 'A_local_accessibility']\n",
    "gdf_hazards['criticality_mean'] = gdf_hazards[norm_cols].mean(axis=1)\n",
    "\n",
    "# Classification function\n",
    "def classify_quintiles(series, labels=None):\n",
    "    if labels is None:\n",
    "        labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "    result = pd.Series('No criticality', index=series.index)\n",
    "    non_zero_mask = series > 0\n",
    "    non_zeros = series[non_zero_mask]\n",
    "    \n",
    "    if not non_zeros.empty:\n",
    "        bins = pd.qcut(non_zeros, 5, labels=labels, duplicates='drop')\n",
    "        result[non_zero_mask] = bins.astype(str)\n",
    "    return result\n",
    "\n",
    "# Apply classification\n",
    "gdf_hazards['mean_class'] = classify_quintiles(gdf_hazards['criticality_mean'])\n",
    "\n",
    "# Ensure Web Mercator for plotting\n",
    "gdf_hazards = gdf_hazards.to_crs(epsg=3857)\n",
    "\n",
    "# Visualization configuration\n",
    "labels = ['No criticality', 'Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "colors = ['#e0e0e0', '#ffffcc', '#a1dab4', '#41b6c4', '#2c7fb8', '#253494']\n",
    "color_map = dict(zip(labels, colors))\n",
    "width_mapping = {\n",
    "    'No criticality': 0.2, 'Very Low': 0.6, 'Low': 1.0, \n",
    "    'Medium': 1.5, 'High': 2.2, 'Very High': 3.2\n",
    "}\n",
    "\n",
    "# Create single figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 9))\n",
    "\n",
    "# Plot each category\n",
    "for cat in labels:\n",
    "    subset = gdf_hazards[gdf_hazards['mean_class'] == cat]\n",
    "    if not subset.empty:\n",
    "        subset.plot(\n",
    "            ax=ax, \n",
    "            color=color_map[cat], \n",
    "            linewidth=width_mapping[cat],\n",
    "            alpha=0.8 if cat != 'No criticality' else 0.4,\n",
    "            zorder=labels.index(cat)\n",
    "        )\n",
    "\n",
    "# Add basemap and remove axes\n",
    "cx.add_basemap(ax, source=cx.providers.CartoDB.Positron, attribution=False)\n",
    "ax.axis('off')\n",
    "\n",
    "# Legend\n",
    "legend_handles = [\n",
    "    Patch(facecolor=color_map[lbl], label=lbl, edgecolor='grey', linewidth=0.5)\n",
    "    for lbl in labels\n",
    "]\n",
    "\n",
    "ax.legend(\n",
    "    handles=legend_handles,\n",
    "    title='Climate Criticality',\n",
    "    loc='upper right',\n",
    "    fontsize=14,\n",
    "    title_fontsize=16,\n",
    "    frameon=True,\n",
    "    framealpha=0.9\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_path / 'climate_criticality_mean.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31401f58-0bab-4f33-92c3-34b096bba622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLIMATE CRITICALITY - SUMMARY STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CLIMATE CRITICALITY SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define classification columns\n",
    "class_cols = ['H_class', 'T_class', 'A_class', 'mean_class']\n",
    "class_names = ['Hazard Exposure', 'Travel Disruption', 'Local Accessibility', 'Combined Criticality']\n",
    "index_cols = ['H_hazard_exposure', 'T_travel_disruption', 'A_local_accessibility', 'criticality_mean']\n",
    "\n",
    "# =============================================================================\n",
    "# 1. OVERALL DISTRIBUTION BY CRITICALITY CLASS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. OVERALL DISTRIBUTION BY CRITICALITY CLASS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "labels = ['No criticality', 'Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "for class_col, class_name in zip(class_cols, class_names):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(\"-\"*50)\n",
    "    counts = gdf_hazards[class_col].value_counts()\n",
    "    total = len(gdf_hazards)\n",
    "    for label in labels:\n",
    "        if label in counts.index:\n",
    "            count = counts[label]\n",
    "            pct = count / total * 100\n",
    "            print(f\"  {label:15s}: {count:5d} ({pct:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  {label:15s}: {0:5d} ({0:5.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CRITICALITY BY ROAD CATEGORY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. CRITICALITY BY ROAD CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cross-tabulation: Road category vs Criticality class\n",
    "for class_col, class_name in zip(class_cols, class_names):\n",
    "    print(f\"\\n{class_name} by Road Category:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Create cross-tab with percentages\n",
    "    ct = pd.crosstab(gdf_hazards['kategorija'], gdf_hazards[class_col], normalize='index') * 100\n",
    "    \n",
    "    # Reorder columns\n",
    "    ct = ct.reindex(columns=[l for l in labels if l in ct.columns])\n",
    "    \n",
    "    print(ct.round(1).to_string())\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MEAN CRITICALITY SCORES BY ROAD CATEGORY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. MEAN CRITICALITY SCORES BY ROAD CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "road_summary = gdf_hazards.groupby('kategorija').agg({\n",
    "    'H_hazard_exposure': ['count', 'mean', 'median', 'max'],\n",
    "    'T_travel_disruption': ['mean', 'median', 'max'],\n",
    "    'A_local_accessibility': ['mean', 'median', 'max'],\n",
    "    'criticality_mean': ['mean', 'median', 'max']\n",
    "}).round(4)\n",
    "\n",
    "print(road_summary.to_string())\n",
    "\n",
    "# =============================================================================\n",
    "# 4. HIGH-TIER vs LOW-TIER ROADS COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. HIGH-TIER vs LOW-TIER ROADS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define road tiers\n",
    "high_tier = ['IA', 'IM', 'IB']  # Motorways and main roads\n",
    "low_tier = ['IIA', 'IIB']       # Regional roads\n",
    "\n",
    "gdf_hazards['road_tier'] = np.where(\n",
    "    gdf_hazards['kategorija'].isin(high_tier), 'High-Tier (IA/IM/IB)',\n",
    "    np.where(gdf_hazards['kategorija'].isin(low_tier), 'Low-Tier (IIA/IIB)', 'Other')\n",
    ")\n",
    "\n",
    "# Compare mean scores\n",
    "print(\"\\nMean Criticality Scores by Road Tier:\")\n",
    "print(\"-\"*60)\n",
    "tier_summary = gdf_hazards.groupby('road_tier')[index_cols].mean().round(4)\n",
    "print(tier_summary.to_string())\n",
    "\n",
    "# Compare distribution of criticality classes\n",
    "print(\"\\n\\nCombined Criticality Distribution by Road Tier (%):\")\n",
    "print(\"-\"*60)\n",
    "tier_ct = pd.crosstab(gdf_hazards['road_tier'], gdf_hazards['mean_class'], normalize='index') * 100\n",
    "tier_ct = tier_ct.reindex(columns=[l for l in labels if l in tier_ct.columns])\n",
    "print(tier_ct.round(1).to_string())\n",
    "\n",
    "# Statistical test (if needed)\n",
    "from scipy import stats\n",
    "high_tier_scores = gdf_hazards[gdf_hazards['road_tier'] == 'High-Tier (IA/IM/IB)']['criticality_mean']\n",
    "low_tier_scores = gdf_hazards[gdf_hazards['road_tier'] == 'Low-Tier (IIA/IIB)']['criticality_mean']\n",
    "\n",
    "if len(high_tier_scores) > 0 and len(low_tier_scores) > 0:\n",
    "    stat, pvalue = stats.mannwhitneyu(high_tier_scores, low_tier_scores, alternative='two-sided')\n",
    "    print(f\"\\nMann-Whitney U test (High-Tier vs Low-Tier):\")\n",
    "    print(f\"  U-statistic: {stat:.2f}\")\n",
    "    print(f\"  p-value: {pvalue:.6f}\")\n",
    "    print(f\"  Significant difference: {'Yes' if pvalue < 0.05 else 'No'}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. TOP 10 MOST CRITICAL SECTIONS (COMBINED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. TOP 10 MOST CRITICAL SECTIONS (Combined Criticality)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_cols = ['oznaka_deo', 'kategorija', 'oznaka_put', 'naziv_poce', 'naziv_zavr',\n",
    "            'H_hazard_exposure', 'T_travel_disruption', 'A_local_accessibility', \n",
    "            'criticality_mean', 'mean_class']\n",
    "top_cols = [c for c in top_cols if c in gdf_hazards.columns]\n",
    "\n",
    "top10_combined = gdf_hazards.nlargest(10, 'criticality_mean')[top_cols]\n",
    "print(top10_combined.to_string())\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TOP 5 PER SUB-INDEX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. TOP 5 SECTIONS PER SUB-INDEX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for index_col, class_name in zip(index_cols[:3], class_names[:3]):\n",
    "    print(f\"\\n{class_name} - Top 5:\")\n",
    "    print(\"-\"*60)\n",
    "    display_cols = ['oznaka_deo', 'kategorija', 'oznaka_put', 'naziv_poce', 'naziv_zavr', index_col]\n",
    "    display_cols = [c for c in display_cols if c in gdf_hazards.columns]\n",
    "    print(gdf_hazards.nlargest(5, index_col)[display_cols].to_string())\n",
    "\n",
    "# =============================================================================\n",
    "# 7. SHARE OF \"HIGH\" + \"VERY HIGH\" BY ROAD CATEGORY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. SHARE OF HIGH/VERY HIGH CRITICALITY BY ROAD CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "high_crit_labels = ['High', 'Very High']\n",
    "\n",
    "for class_col, class_name in zip(class_cols, class_names):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Calculate % of High + Very High per road category\n",
    "    high_crit_pct = gdf_hazards.groupby('kategorija').apply(\n",
    "        lambda x: (x[class_col].isin(high_crit_labels)).sum() / len(x) * 100\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Road Category | % High/Very High | Count High/Very High | Total\")\n",
    "    for cat in high_crit_pct.index:\n",
    "        pct = high_crit_pct[cat]\n",
    "        count_high = gdf_hazards[(gdf_hazards['kategorija'] == cat) & \n",
    "                                  (gdf_hazards[class_col].isin(high_crit_labels))].shape[0]\n",
    "        total = gdf_hazards[gdf_hazards['kategorija'] == cat].shape[0]\n",
    "        print(f\"  {cat:12s} | {pct:14.1f}% | {count_high:20d} | {total}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. CORRELATION BETWEEN SUB-INDICES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. CORRELATION BETWEEN SUB-INDICES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "corr_matrix = gdf_hazards[index_cols].corr()\n",
    "print(corr_matrix.round(3).to_string())\n",
    "\n",
    "# =============================================================================\n",
    "# 9. SUMMARY TABLE FOR TEXT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. SUMMARY TABLE FOR TEXT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "for cat in gdf_hazards['kategorija'].dropna().unique():\n",
    "    subset = gdf_hazards[gdf_hazards['kategorija'] == cat]\n",
    "    n_sections = len(subset)\n",
    "    \n",
    "    # % in High/Very High for each sub-index\n",
    "    pct_H_high = (subset['H_class'].isin(high_crit_labels)).sum() / n_sections * 100\n",
    "    pct_T_high = (subset['T_class'].isin(high_crit_labels)).sum() / n_sections * 100\n",
    "    pct_A_high = (subset['A_class'].isin(high_crit_labels)).sum() / n_sections * 100\n",
    "    pct_CC_high = (subset['mean_class'].isin(high_crit_labels)).sum() / n_sections * 100\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Road Category': cat,\n",
    "        'N Sections': n_sections,\n",
    "        '% High/VH Hazard': f\"{pct_H_high:.1f}%\",\n",
    "        '% High/VH Travel': f\"{pct_T_high:.1f}%\",\n",
    "        '% High/VH Access': f\"{pct_A_high:.1f}%\",\n",
    "        '% High/VH Combined': f\"{pct_CC_high:.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('Road Category')\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# 10. KEY FINDINGS FOR TEXT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. KEY FINDINGS FOR TEXT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_sections = len(gdf_hazards)\n",
    "high_vh_combined = gdf_hazards['mean_class'].isin(high_crit_labels).sum()\n",
    "pct_high_vh = high_vh_combined / total_sections * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "KEY STATISTICS:\n",
    "- Total road sections analyzed: {total_sections}\n",
    "- Sections with High/Very High combined criticality: {high_vh_combined} ({pct_high_vh:.1f}%)\n",
    "\n",
    "ROAD TIER COMPARISON:\n",
    "- High-tier roads (IA/IM/IB) mean criticality: {high_tier_scores.mean():.4f}\n",
    "- Low-tier roads (IIA/IIB) mean criticality: {low_tier_scores.mean():.4f}\n",
    "\n",
    "TOP CRITICAL SECTION:\n",
    "\"\"\")\n",
    "\n",
    "top1 = gdf_hazards.nlargest(1, 'criticality_mean').iloc[0]\n",
    "print(f\"  Road: {top1.get('oznaka_put', 'N/A')} ({top1.get('kategorija', 'N/A')})\")\n",
    "print(f\"  Section: {top1.get('naziv_poce', 'N/A')} → {top1.get('naziv_zavr', 'N/A')}\")\n",
    "print(f\"  Combined Criticality Score: {top1['criticality_mean']:.4f}\")\n",
    "print(f\"  H: {top1['H_hazard_exposure']:.4f}, T: {top1['T_travel_disruption']:.4f}, A: {top1['A_local_accessibility']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4450964-b918-4486-aa7b-bbc11bf2c476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00988eba-b9ab-4fc6-9c2a-6d942681ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hazards.to_excel(intermediate_results_path / 'VUA_Climate_Criticality_PERS.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83259773-22f8-46bc-96e2-f1db414707b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_hazards.sort_values('criticality_sum',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db99874-8f59-41f6-bcb2-728aab16a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare data: drop zero or missing criticality\n",
    "gdf_plot = gdf_hazards.copy().to_crs(3857)\n",
    "gdf_plot = gdf_plot[gdf_plot['criticality_sum'].fillna(0) > 0.1]\n",
    "\n",
    "# 2) Define bins and labels\n",
    "bins = [0, 1, 2, 3, np.inf]\n",
    "labels = ['0–1', '1–2', '2–3', '>3']\n",
    "\n",
    "# 3) Assign classes\n",
    "gdf_plot['criticality_class'] = pd.cut(\n",
    "    gdf_plot['criticality_sum'],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    include_lowest=False,  # (0–1], (1–2], (2–3], (3, inf)\n",
    "    right=True\n",
    ")\n",
    "\n",
    "# 4) Colors (low to high) — swap to your preferred palette if needed\n",
    "colors = ['#fcae91','#fb6a4a','#de2d26','#a50f15']\n",
    "color_map = dict(zip(labels, colors))\n",
    "\n",
    "# 5) Figure and axis\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 9), facecolor='white')\n",
    "\n",
    "# Base network (light grey) — plotted above basemap, below criticality\n",
    "base_network.to_crs(3857).plot(ax=ax, linewidth=0.15, color='lightgrey', alpha=0.6, zorder=2)\n",
    "\n",
    "# Plot each class\n",
    "for cls in labels:\n",
    "    sub = gdf_plot[gdf_plot['criticality_class'] == cls]\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    sub.plot(\n",
    "        ax=ax,\n",
    "        color=color_map[cls],\n",
    "        linewidth=1.5,\n",
    "        alpha=0.95,\n",
    "        zorder=4\n",
    "    )\n",
    "\n",
    "# Legend\n",
    "legend_handles = [\n",
    "    Patch(facecolor=color_map[lbl], edgecolor='black', linewidth=0.3, label=lbl)\n",
    "    for lbl in labels\n",
    "]\n",
    "ax.legend(\n",
    "    handles=legend_handles,\n",
    "    title='Criticality (sum)',\n",
    "    loc='upper right',\n",
    "    frameon=True,\n",
    "    fancybox=True,\n",
    "    framealpha=0.9,\n",
    "    fontsize=12,\n",
    "    title_fontsize=14\n",
    ")\n",
    "\n",
    "cx.add_basemap(\n",
    "    ax=ax,\n",
    "    source=cx.providers.CartoDB.Positron,\n",
    "    alpha=0.8,       # light background\n",
    "    attribution=False,\n",
    "    zorder=1)\n",
    "\n",
    "\n",
    "# Cosmetics\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "plt.savefig(figure_path / 'criticality_sum_map.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3119b8-93c5-422a-9ed3-bc3557d08946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Settings ---\n",
    "TOP_METHOD = \"quantile\"    # \"quantile\" or \"topN\" or \"cutoff\"\n",
    "TOP_Q = 0.90               # top 10% by criticality_sum\n",
    "TOP_N = 1000               # if TOP_METHOD == \"topN\"\n",
    "CUTOFF_VAL = 3.0           # if TOP_METHOD == \"cutoff\" (e.g., >3 in your legend)\n",
    "\n",
    "# --- Prep ---\n",
    "gdf = gdf_hazards.copy()\n",
    "\n",
    "gdf[\"len_km\"] = gdf['road_length']\n",
    "\n",
    "# Identify normalized submetrics used in the sum\n",
    "norm_cols = sorted([c for c in gdf.columns if c.endswith(\"_norm\")])\n",
    "if \"criticality_sum\" not in gdf.columns:\n",
    "    # If not present, create it as sum of all _norm columns\n",
    "    gdf[\"criticality_sum\"] = gdf[norm_cols].sum(axis=1)\n",
    "\n",
    "# --- Select top critical segments ---\n",
    "if TOP_METHOD == \"quantile\":\n",
    "    thr = gdf[\"criticality_sum\"].quantile(TOP_Q)\n",
    "    top_mask = gdf[\"criticality_sum\"] >= thr\n",
    "elif TOP_METHOD == \"topN\":\n",
    "    top_idx = gdf[\"criticality_sum\"].nlargest(TOP_N).index\n",
    "    top_mask = gdf.index.isin(top_idx)\n",
    "elif TOP_METHOD == \"cutoff\":\n",
    "    top_mask = gdf[\"criticality_sum\"] > CUTOFF_VAL\n",
    "else:\n",
    "    raise ValueError(\"TOP_METHOD must be one of: quantile, topN, cutoff\")\n",
    "\n",
    "top = gdf.loc[top_mask].copy()\n",
    "rest = gdf.loc[~top_mask].copy()\n",
    "\n",
    "print(f\"Total segments: {len(gdf):,}\")\n",
    "print(f\"Top subset size: {len(top):,} ({len(top)/len(gdf)*100:.1f}%)\")\n",
    "print(f\"Top subset total length: {top['len_km'].sum():,.1f} km\")\n",
    "print(f\"Mean criticality_sum (top): {top['criticality_sum'].mean():.2f}\")\n",
    "print(f\"Mean criticality_sum (rest): {rest['criticality_sum'].mean():.2f}\")\n",
    "\n",
    "# --- 1) Which road categories dominate in the top subset? ---\n",
    "cat_summary_top = (top.groupby(\"kategorija\")\n",
    "                      .agg(n_segments=(\"kategorija\", \"size\"),\n",
    "                           length_km=(\"len_km\", \"sum\"),\n",
    "                           avg_crit=(\"criticality_sum\", \"mean\"))\n",
    "                      .sort_values([\"n_segments\", \"length_km\"], ascending=False))\n",
    "cat_summary_all = (gdf.groupby(\"kategorija\")\n",
    "                      .agg(n_segments=(\"kategorija\", \"size\"),\n",
    "                           length_km=(\"len_km\", \"sum\"))\n",
    "                      .sort_values([\"n_segments\", \"length_km\"], ascending=False))\n",
    "\n",
    "# Add shares vs. network totals\n",
    "cat_summary_top[\"share_count_%\"] = (cat_summary_top[\"n_segments\"] / len(top) * 100).round(1)\n",
    "cat_summary_top[\"share_length_%\"] = (cat_summary_top[\"length_km\"] / top[\"len_km\"].sum() * 100).round(1)\n",
    "\n",
    "print(\"\\nTop critical subset by road category (count, length, and avg criticality):\")\n",
    "print(cat_summary_top.round(2).to_string())\n",
    "\n",
    "print(\"\\nAll segments by road category (for baseline context):\")\n",
    "print(cat_summary_all.round(2).to_string())\n",
    "\n",
    "# --- 2) Which submetrics contributed most to top rankings (overall)? ---\n",
    "# Contribution here is the sum of normalized values per column in the top subset.\n",
    "submetric_contrib_top = top[norm_cols].sum().sort_values(ascending=False)\n",
    "submetric_contrib_top_pct = (submetric_contrib_top / submetric_contrib_top.sum() * 100).round(1)\n",
    "submetric_contrib_df = pd.DataFrame({\n",
    "    \"total_contribution\": submetric_contrib_top.round(2),\n",
    "    \"share_%\": submetric_contrib_top_pct\n",
    "}).sort_values(\"total_contribution\", ascending=False)\n",
    "\n",
    "print(\"\\nSubmetric contributions in top subset (overall):\")\n",
    "print(submetric_contrib_df.to_string())\n",
    "\n",
    "# --- 3) Which submetric dominates within each road category (top subset)? ---\n",
    "# For each category, compute contribution shares across submetrics\n",
    "by_cat_contrib = (top.groupby(\"kategorija\")[norm_cols].sum())\n",
    "by_cat_contrib_share = by_cat_contrib.div(by_cat_contrib.sum(axis=1), axis=0) * 100.0\n",
    "# Identify dominant submetric per category\n",
    "dominant_metric_per_cat = by_cat_contrib_share.idxmax(axis=1).to_frame(\"dominant_submetric\")\n",
    "dominant_share_per_cat = by_cat_contrib_share.max(axis=1).round(1).to_frame(\"dominant_share_%\")\n",
    "dominant_cat_df = (pd.concat([dominant_metric_per_cat, dominant_share_per_cat], axis=1)\n",
    "                     .sort_values(\"dominant_share_%\", ascending=False))\n",
    "\n",
    "print(\"\\nDominant submetric per road category in the top subset:\")\n",
    "print(dominant_cat_df.to_string())\n",
    "\n",
    "# --- 4) Quick list of top segments (IDs) with their category and contributions ---\n",
    "# Show top 20 by criticality_sum with breakdown across submetrics\n",
    "top_breakdown = top.loc[top[\"criticality_sum\"].nlargest(20).index,\n",
    "                        [\"kategorija\", \"criticality_sum\"] + norm_cols].round(3)\n",
    "print(\"\\nTop 20 segments by criticality_sum with submetric breakdown:\")\n",
    "print(top_breakdown.to_string())\n",
    "\n",
    "# --- 5) Optional: overall shares by band used in your map legend ---\n",
    "bands = pd.cut(gdf[\"criticality_sum\"], bins=[0,1,2,3,np.inf], labels=[\"0–1\",\"1–2\",\"2–3\",\">3\"], right=True)\n",
    "band_share = bands.value_counts(normalize=True).sort_index() * 100\n",
    "print(\"\\nNetwork share by criticality_sum bands (0–1, 1–2, 2–3, >3):\")\n",
    "print(band_share.round(1).to_string())\n",
    "\n",
    "# --- 6) Optional: per-category median criticality and length in the top subset ---\n",
    "cat_medians = top.groupby(\"kategorija\").agg(\n",
    "    median_crit=(\"criticality_sum\", \"median\"),\n",
    "    length_km=(\"len_km\", \"sum\")\n",
    ").sort_values(\"median_crit\", ascending=False)\n",
    "print(\"\\nPer-category median criticality and total length (top subset):\")\n",
    "print(cat_medians.round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398880e-fc2e-46cc-9f13-2db7edff9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vehicle Hours Lost (VHL) fun facts for the top subset ---\n",
    "\n",
    "def _find_col(df, candidates):\n",
    "    \"\"\"Return the first column found in df among candidates, case-insensitive.\"\"\"\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "# 1) Identify or construct raw VHL\n",
    "# Try existing raw VHL first\n",
    "vhl_col = _find_col(gdf, [\"VHL\", \"vhl\", \"vehicle_hours_lost\", \"vehicle_hours_lost_daily\", \"vhl_daily\"])\n",
    "\n",
    "# Else compute from delay and AADT if available\n",
    "delay_col = _find_col(gdf, [\"avg_delay_min\", \"avg_time_disruption_min\", \"delay_min\", \"avg_delay_minutes\"])\n",
    "aadt_col  = _find_col(gdf, [\"AADT\", \"aadt\"])\n",
    "\n",
    "if vhl_col is None and (delay_col is not None and aadt_col is not None):\n",
    "    vhl_col = \"VHL_computed_daily\"\n",
    "    gdf[vhl_col] = (gdf[delay_col].astype(float) / 60.0) * gdf[aadt_col].astype(float)\n",
    "\n",
    "# Fallback to normalized proxy only if nothing else is available\n",
    "using_normalized_proxy = False\n",
    "if vhl_col is None:\n",
    "    vhl_norm_col = _find_col(gdf, [\"vhl_norm\"])\n",
    "    if vhl_norm_col is None:\n",
    "        raise ValueError(\"Could not find raw VHL nor ingredients to compute it, and vhl_norm is also missing.\")\n",
    "    vhl_col = vhl_norm_col\n",
    "    using_normalized_proxy = True\n",
    "\n",
    "# 2) Prepare series for top and rest\n",
    "vhl_top = top[vhl_col].astype(float)\n",
    "vhl_rest = rest[vhl_col].astype(float)\n",
    "vhl_all = gdf[vhl_col].astype(float)\n",
    "\n",
    "# 3) Core fun facts\n",
    "def pct(x): \n",
    "    return f\"{100.0 * x:.1f}%\"\n",
    "\n",
    "if not using_normalized_proxy:\n",
    "    # Thresholds in daily vehicle-hours\n",
    "    thresholds = [1000, 5000, 10000]\n",
    "\n",
    "    share_total_vhl_top = (vhl_top.sum() / vhl_all.sum()) if vhl_all.sum() > 0 else np.nan\n",
    "    med_top = np.nanmedian(vhl_top)\n",
    "    p90_top = np.nanpercentile(vhl_top, 90)\n",
    "    p95_top = np.nanpercentile(vhl_top, 95)\n",
    "    max_top = np.nanmax(vhl_top)\n",
    "\n",
    "    print(\"\\n--- Vehicle hours lost: ready-to-paste statements ---\")\n",
    "    print(f\"The top critical subset accounts for {pct(share_total_vhl_top)} of total daily vehicle hours lost on the network.\")\n",
    "    print(f\"Within the top subset the median daily vehicle hours lost per segment is {med_top:,.0f}, with the 90th and 95th percentiles at {p90_top:,.0f} and {p95_top:,.0f}, and a maximum of {max_top:,.0f}.\")\n",
    "\n",
    "    # Counts above thresholds in top vs rest\n",
    "    for thr in thresholds:\n",
    "        n_top = int((vhl_top >= thr).sum())\n",
    "        n_rest = int((vhl_rest >= thr).sum())\n",
    "        print(f\"{n_top} segments in the top subset exceed {thr:,.0f} daily vehicle hours lost \"\n",
    "              f\"(compared with {n_rest} in the rest of the network).\")\n",
    "\n",
    "    # Which road classes most often exceed thresholds\n",
    "    top_with_vhl = top.assign(_vhl=vhl_top.values).copy()\n",
    "    for thr in thresholds:\n",
    "        exceed = (top_with_vhl[top_with_vhl[\"_vhl\"] >= thr]\n",
    "                  .groupby(\"kategorija\")\n",
    "                  .size()\n",
    "                  .sort_values(ascending=False))\n",
    "        if exceed.empty:\n",
    "            continue\n",
    "        print(f\"\\nAmong top segments exceeding {thr:,.0f} daily vehicle hours lost, the most frequent categories are:\")\n",
    "        for cat, cnt in exceed.items():\n",
    "            print(f\"  {cat}: {cnt} segments\")\n",
    "\n",
    "else:\n",
    "    # Normalized proxy mode: use quantiles rather than absolute thresholds\n",
    "    share_total_vhl_top = (vhl_top.sum() / vhl_all.sum()) if vhl_all.sum() > 0 else np.nan\n",
    "    med_top = np.nanmedian(vhl_top)\n",
    "    p90_top = np.nanpercentile(vhl_top, 90)\n",
    "    p95_top = np.nanpercentile(vhl_top, 95)\n",
    "    max_top = np.nanmax(vhl_top)\n",
    "\n",
    "    print(\"\\n--- Vehicle hours lost (normalized proxy): ready-to-paste statements ---\")\n",
    "    print(f\"The top critical subset accounts for {pct(share_total_vhl_top)} of the summed normalized vehicle hours lost metric.\")\n",
    "    print(f\"Within the top subset the median normalized value is {med_top:,.3f}, with the 90th and 95th percentiles at {p90_top:,.3f} and {p95_top:,.3f}, and a maximum of {max_top:,.3f}.\")\n",
    "\n",
    "    # Report which categories dominate the top decile of the proxy\n",
    "    q_thr = np.nanpercentile(vhl_all, 90)\n",
    "    exceed = (top[vhl_col] >= q_thr)\n",
    "    by_cat = top.loc[exceed].groupby(\"kategorija\").size().sort_values(ascending=False)\n",
    "    if not by_cat.empty:\n",
    "        print(\"\\nWithin the top subset, categories most represented in the top decile of the normalized vehicle hours lost proxy are:\")\n",
    "        for cat, cnt in by_cat.items():\n",
    "            print(f\"  {cat}: {cnt} segments\")\n",
    "\n",
    "# 4) Optional: Top segments by VHL for quick cross-checking\n",
    "n_show = 20\n",
    "cols_show = [\"kategorija\", \"criticality_sum\"]\n",
    "if vhl_col not in cols_show:\n",
    "    cols_show = [\"kategorija\", \"criticality_sum\", vhl_col]\n",
    "top_by_vhl = top.sort_values(by=vhl_col, ascending=False).head(n_show)[cols_show]\n",
    "print(f\"\\nTop {n_show} segments by {vhl_col}:\")\n",
    "print(top_by_vhl.to_string(index=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serbia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
